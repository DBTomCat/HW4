{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Persian Poetry FineTuning.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7421720,"sourceType":"datasetVersion","datasetId":4318074}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport os\nimport glob\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nfrom pathlib import Path\n\nimport torch\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelWithLMHead\nfrom transformers import AutoTokenizer, GPT2LMHeadModel, GPT2Config\n\nfrom IPython import display","metadata":{"id":"m_2U9J1o5Gtu","execution":{"iopub.status.busy":"2024-01-17T19:02:10.012521Z","iopub.execute_input":"2024-01-17T19:02:10.012914Z","iopub.status.idle":"2024-01-17T19:02:13.272358Z","shell.execute_reply.started":"2024-01-17T19:02:10.012856Z","shell.execute_reply":"2024-01-17T19:02:13.271556Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/ferdousi-poems/ferdousi.txt', 'r') as data:\n    data = data.readlines()[2:]\n    out_file = open('ferdousi_modified.csv', 'w')\n    output = ['first\\tlast\\n']\n    for i in range(0, len(data)-1, 2):\n        output.append(data[i].rstrip()+'\\t' + data[i + 1].rstrip()+'\\n')\n    out_file.writelines(output)\n    out_file.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T19:02:13.274187Z","iopub.execute_input":"2024-01-17T19:02:13.274627Z","iopub.status.idle":"2024-01-17T19:02:13.394890Z","shell.execute_reply.started":"2024-01-17T19:02:13.274596Z","shell.execute_reply":"2024-01-17T19:02:13.393808Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"ferdousi_modified.csv\", sep=\"\\t\")\n\ndf[\"beits\"] = df[\"first\"].values + \"<sep>\" + df[\"last\"]\n\ntexts = df[\"beits\"].values.tolist()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"Osc729oi5U1w","outputId":"d699c0ef-d88a-4b31-fed7-89f74d3446e2","execution":{"iopub.status.busy":"2024-01-17T19:02:13.395974Z","iopub.execute_input":"2024-01-17T19:02:13.396273Z","iopub.status.idle":"2024-01-17T19:02:13.556558Z","shell.execute_reply.started":"2024-01-17T19:02:13.396248Z","shell.execute_reply":"2024-01-17T19:02:13.555711Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df[\"beits\"][:5]","metadata":{"execution":{"iopub.status.busy":"2024-01-17T19:02:13.558766Z","iopub.execute_input":"2024-01-17T19:02:13.559373Z","iopub.status.idle":"2024-01-17T19:02:13.568202Z","shell.execute_reply.started":"2024-01-17T19:02:13.559343Z","shell.execute_reply":"2024-01-17T19:02:13.567163Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0    به نام خداوند جان و خرد<sep>کزین برتر اندیشه ب...\n1    خداوند نام و خداوند جای<sep>خداوند روزی ده رهنمای\n2    خداوند کیوان و گردان سپهر<sep>فروزنده ماه و نا...\n3    ز نام و نشان و گمان برترست<sep>نگارندهٔ بر شده...\n4    به بینندگان آفریننده را<sep>نبینی مرنجان دو بی...\nName: beits, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"model_name_or_path = \"HooshvareLab/gpt2-fa\"\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name_or_path,\n    bos_token='<s>', \n    eos_token='</s>', \n    pad_token='<pad>',\n    unk_token='<unk>'\n)\ntokenizer.add_special_tokens({\n    \"bos_token\": '</s>',\n    \"eos_token\": '</s>', \n    \"pad_token\": '<pad>',\n    \"unk_token\": '<unk>'\n})\n\nconfig = AutoConfig.from_pretrained(\n    model_name_or_path,\n    bos_token_id=tokenizer(\"<s>\")[\"input_ids\"][0], \n    eos_token_id=tokenizer(\"</s>\")[\"input_ids\"][0], \n    pad_token_id=tokenizer(\"<pad>\")[\"input_ids\"][0],\n    unk_token_id=tokenizer(\"<unk>\")[\"input_ids\"][0],\n)\n\ntokenizer.save_pretrained(\"/content/gpt2/\")\nconfig.save_pretrained(\"/content/gpt2/\")\n\n!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/pytorch_model.bin\" -P /content/gpt2/\n!wget \"https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/tokenizer.json\" -P /content/gpt2/","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RdnnYyAeEt5H","outputId":"c3cc7b86-91e4-46e5-c98a-b5d72d383b84","execution":{"iopub.status.busy":"2024-01-17T19:02:13.569870Z","iopub.execute_input":"2024-01-17T19:02:13.570639Z","iopub.status.idle":"2024-01-17T19:02:17.947371Z","shell.execute_reply.started":"2024-01-17T19:02:13.570602Z","shell.execute_reply":"2024-01-17T19:02:17.946149Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--2024-01-17 19:02:14--  https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/pytorch_model.bin\nResolving huggingface.co (huggingface.co)... 18.244.202.118, 18.244.202.73, 18.244.202.60, ...\nConnecting to huggingface.co (huggingface.co)|18.244.202.118|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.huggingface.co/HooshvareLab/gpt2-fa/46b0b806c740a0f0a9f056f5574c5fa896166fe844945fd3c849bf34365e5060?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1705777334&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTc3NzMzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9Ib29zaHZhcmVMYWIvZ3B0Mi1mYS80NmIwYjgwNmM3NDBhMGYwYTlmMDU2ZjU1NzRjNWZhODk2MTY2ZmU4NDQ5NDVmZDNjODQ5YmYzNDM2NWU1MDYwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=AQkTRiKLMkJdS-vl88Enp21ZuWE9wiw2J6wwekRsv20GH2r2SXzUauLcW9RDGeOtXupLgvhyCTq7IlQNwWvMAjvIr-6-aJT8SMYbtiGZfhsEXrwWP5BFIT90ONYLagqjnbOsxceN9uKvColhFH7Aaff2oSZNLfxsD%7ELGryXmOqWMEuetKrUeI1bjXbzYFbL%7EiT478Tu%7Edu0t21E%7E11lWAuqQqOQB9P-lcJEynaaNAMKqDDf1aL%7EAofIPDbeBvSwp0mCYWmEv4ZBNvWaBAd1ggz4rIzkWjUvockIQEn4aob7MQgRd%7EED1EyuccFn3iaz6RXqeOYgha8DDaWewB1Y0og__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n--2024-01-17 19:02:15--  https://cdn-lfs.huggingface.co/HooshvareLab/gpt2-fa/46b0b806c740a0f0a9f056f5574c5fa896166fe844945fd3c849bf34365e5060?response-content-disposition=attachment%3B+filename*%3DUTF-8''pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1705777334&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwNTc3NzMzNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9Ib29zaHZhcmVMYWIvZ3B0Mi1mYS80NmIwYjgwNmM3NDBhMGYwYTlmMDU2ZjU1NzRjNWZhODk2MTY2ZmU4NDQ5NDVmZDNjODQ5YmYzNDM2NWU1MDYwP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=AQkTRiKLMkJdS-vl88Enp21ZuWE9wiw2J6wwekRsv20GH2r2SXzUauLcW9RDGeOtXupLgvhyCTq7IlQNwWvMAjvIr-6-aJT8SMYbtiGZfhsEXrwWP5BFIT90ONYLagqjnbOsxceN9uKvColhFH7Aaff2oSZNLfxsD~LGryXmOqWMEuetKrUeI1bjXbzYFbL~iT478Tu~du0t21E~11lWAuqQqOQB9P-lcJEynaaNAMKqDDf1aL~AofIPDbeBvSwp0mCYWmEv4ZBNvWaBAd1ggz4rIzkWjUvockIQEn4aob7MQgRd~ED1EyuccFn3iaz6RXqeOYgha8DDaWewB1Y0og__&Key-Pair-Id=KVTP0A1DKRTAX\nResolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 18.244.202.14, 18.244.202.105, 18.244.202.35, ...\nConnecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|18.244.202.14|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 485044198 (463M) [application/octet-stream]\nSaving to: '/content/gpt2/pytorch_model.bin.3'\n\npytorch_model.bin.3 100%[===================>] 462.57M   295MB/s    in 1.6s    \n\n2024-01-17 19:02:16 (295 MB/s) - '/content/gpt2/pytorch_model.bin.3' saved [485044198/485044198]\n\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--2024-01-17 19:02:17--  https://huggingface.co/HooshvareLab/gpt2-fa/resolve/main/tokenizer.json\nResolving huggingface.co (huggingface.co)... 18.244.202.118, 18.244.202.73, 18.244.202.68, ...\nConnecting to huggingface.co (huggingface.co)|18.244.202.118|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2748949 (2.6M) [text/plain]\nSaving to: '/content/gpt2/tokenizer.json.4'\n\ntokenizer.json.4    100%[===================>]   2.62M  --.-KB/s    in 0.1s    \n\n2024-01-17 19:02:17 (26.0 MB/s) - '/content/gpt2/tokenizer.json.4' saved [2748949/2748949]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\n    \"/content/gpt2\",\n    bos_token='<s>', \n    eos_token='</s>', \n    pad_token='<pad>'\n)\n\nprint(tokenizer.encode(\"سلام بر شما\"))\nprint(tokenizer.encode(\"<s>\"))\nprint(tokenizer.encode(\"</s>\"))\nprint(tokenizer.encode(\"<pad>\"))\nprint(tokenizer.encode(\"<|startoftext|>\"))\nprint(tokenizer.encode(\"<sep>\"))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_rOYPUa9GWuI","outputId":"03caf027-e77a-4a59-849f-49aec21f3d70","execution":{"iopub.status.busy":"2024-01-17T19:02:17.949164Z","iopub.execute_input":"2024-01-17T19:02:17.949556Z","iopub.status.idle":"2024-01-17T19:02:18.051540Z","shell.execute_reply.started":"2024-01-17T19:02:17.949520Z","shell.execute_reply":"2024-01-17T19:02:18.050587Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[8906, 327, 512]\n[0]\n[2]\n[1]\n[6]\n[9]\n","output_type":"stream"}]},{"cell_type":"code","source":"seq_lengths = [len(tokenizer.encode(text)) for text in texts]\nseq_lengths = list(np.array(seq_lengths[:-1]) + np.array(seq_lengths[1:]))\nmax_seq = max(seq_lengths)+len([\"<s>\",\"<|startoftext|>\",\"</s>\"])\nprint(f'The longest dataset input is {max_seq} tokens long.')","metadata":{"id":"19fApJsoIJA1","execution":{"iopub.status.busy":"2024-01-17T19:02:18.052717Z","iopub.execute_input":"2024-01-17T19:02:18.053026Z","iopub.status.idle":"2024-01-17T19:02:22.868340Z","shell.execute_reply.started":"2024-01-17T19:02:18.053000Z","shell.execute_reply":"2024-01-17T19:02:22.867155Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"The longest dataset input is 44 tokens long.\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import Dataset  # this is the pytorch class import\nimport torch\ntorch.manual_seed(42)\n\n\nclass MTGDataset(Dataset):\n\n    def __init__(self, txt_list, tokenizer, max_length=1024):\n\n        self.tokenizer = tokenizer  # the gpt2 tokenizer we instantiated\n        self.input_ids = []\n        self.attn_masks = []\n        \n        \n        for i in range(len(txt_list)-1):\n            \"\"\"\n            This loop will iterate through each entry in the flavour text corpus.\n            For each bit of text it will prepend it with the start of text token,\n            then append the end of text token and pad to the maximum length with the \n            pad token. \n            \"\"\"\n            this=txt_list[i]\n            that=txt_list[i+1]\n            encodings_dict = tokenizer(\"<s>\"+this+\"<|startoftext|>\"+that+\"</s>\",\n                                       truncation=True,\n                                       max_length=max_length,\n                                       padding=\"max_length\")\n\n            \"\"\"\n            Each iteration then appends either the encoded tensor to a list,\n            or the attention mask for that encoding to a list. The attention mask is\n            a binary list of 1's or 0's which determine whether the langauge model\n            should take that token into consideration or not. \n            \"\"\"\n            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n\n    def __len__(self):\n        return len(self.input_ids)-1\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.attn_masks[idx]","metadata":{"id":"C5m3YNOvIKoU","execution":{"iopub.status.busy":"2024-01-17T19:02:22.869863Z","iopub.execute_input":"2024-01-17T19:02:22.870270Z","iopub.status.idle":"2024-01-17T19:02:22.886825Z","shell.execute_reply.started":"2024-01-17T19:02:22.870233Z","shell.execute_reply":"2024-01-17T19:02:22.885816Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ndataset = MTGDataset(texts, tokenizer, max_length=max_seq)\n\n# Split into training and validation sets\ntrain_size = int(0.9 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\nf'There are {len(train_dataset)} samples for training, and {len(val_dataset)} samples for validation testing'","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MwY3srMLINfs","outputId":"3e44e0e1-e157-4964-f15a-bf0d0016aec5","execution":{"iopub.status.busy":"2024-01-17T19:02:22.888283Z","iopub.execute_input":"2024-01-17T19:02:22.888677Z","iopub.status.idle":"2024-01-17T19:02:37.055911Z","shell.execute_reply.started":"2024-01-17T19:02:22.888638Z","shell.execute_reply":"2024-01-17T19:02:37.054776Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'There are 44645 samples for training, and 4961 samples for validation testing'"},"metadata":{}}]},{"cell_type":"code","source":"print(tokenizer.decode(train_dataset[0][0]).replace(\"<sep>\",\"\\t\").replace(\"<|startoftext|>\",\"\\n\").replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\"))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lplxA4inIReU","outputId":"dd94b531-7322-4b8b-c1c8-1b24b4ead84d","execution":{"iopub.status.busy":"2024-01-17T19:02:37.059446Z","iopub.execute_input":"2024-01-17T19:02:37.059745Z","iopub.status.idle":"2024-01-17T19:02:37.066433Z","shell.execute_reply.started":"2024-01-17T19:02:37.059719Z","shell.execute_reply":"2024-01-17T19:02:37.065369Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"سپاسی برین کار بر من نهی\tکز اندیشه گردد دل من تهی\nبدو گفت رستم که چندین سخن\tکه گفتی و افگندی از مهر بن\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ntrain_dataloader = DataLoader(\n    train_dataset,\n    sampler=RandomSampler(train_dataset),\n    batch_size=16\n)\n\nvalidation_dataloader = DataLoader(\n    val_dataset,\n    sampler=SequentialSampler(val_dataset),\n    batch_size=16\n)","metadata":{"id":"8hxgySomITYV","execution":{"iopub.status.busy":"2024-01-17T19:02:37.067628Z","iopub.execute_input":"2024-01-17T19:02:37.068004Z","iopub.status.idle":"2024-01-17T19:02:37.076618Z","shell.execute_reply.started":"2024-01-17T19:02:37.067967Z","shell.execute_reply":"2024-01-17T19:02:37.075598Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import random\nfrom transformers import GPT2LMHeadModel, GPT2Config\nimport numpy as np\n\n# Loading the model configuration and setting it to the GPT2 standard settings.\nconfiguration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\n\n# Create the instance of the model and set the token size embedding length\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Tell pytorch to run this model on the GPU.\ndevice = torch.device(\"cuda\")\nmodel.cuda()\n\n# This step is optional but will enable reproducible runs.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","metadata":{"id":"TVUEzpPvIYYV","execution":{"iopub.status.busy":"2024-01-17T19:02:37.077896Z","iopub.execute_input":"2024-01-17T19:02:37.078239Z","iopub.status.idle":"2024-01-17T19:02:38.638011Z","shell.execute_reply.started":"2024-01-17T19:02:37.078207Z","shell.execute_reply":"2024-01-17T19:02:38.637146Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nwarmup_steps = 1e2\nsample_every = 1000\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=5e-4,\n    eps=1e-8\n)","metadata":{"id":"2zgo4J21IaYc","execution":{"iopub.status.busy":"2024-01-17T19:02:38.639232Z","iopub.execute_input":"2024-01-17T19:02:38.639540Z","iopub.status.idle":"2024-01-17T19:02:38.645291Z","shell.execute_reply.started":"2024-01-17T19:02:38.639513Z","shell.execute_reply":"2024-01-17T19:02:38.644425Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\ntotal_steps = len(train_dataloader) * epochs\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=warmup_steps,\n    num_training_steps=total_steps)\n","metadata":{"id":"uiWGu9MsIcw8","execution":{"iopub.status.busy":"2024-01-17T19:02:38.646430Z","iopub.execute_input":"2024-01-17T19:02:38.646708Z","iopub.status.idle":"2024-01-17T19:02:38.655499Z","shell.execute_reply.started":"2024-01-17T19:02:38.646684Z","shell.execute_reply":"2024-01-17T19:02:38.654531Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"import random\nimport time\nimport datetime\nfrom tqdm import tqdm\n\n\ndef format_time(elapsed):\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n\n\ntotal_t0 = time.time()\n\ntraining_stats = []\n\nmodel = model.to(device)\n\nfor epoch_i in tqdm(range(0, epochs), position=0):\n\n    print(f'Beginning epoch {epoch_i + 1} of {epochs}')\n\n    t0 = time.time()\n\n    total_train_loss = 0\n\n    model.train()\n\n    for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader), position=0):\n\n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n\n        model.zero_grad()\n\n        outputs = model(b_input_ids, labels=b_labels, attention_mask=b_masks)#, token_type_ids=None)\n\n        loss = outputs[0]\n\n        batch_loss = loss.item()\n        total_train_loss += batch_loss\n\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        \n        if step % sample_every == 0 and not step == 0:\n\n            elapsed = format_time(time.time() - t0)\n            print()\n            print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')\n\n            model.eval()\n\n\n            sample_input = train_dataset[random.randint(0, len(train_dataset))]\n            sample_input_tokens = sample_input[0][:torch.where(sample_input[0] == 6)[0][0]+1]\n            sample_input_ids = sample_input_tokens.unsqueeze(0).to(device)# sample_input_ids.to(device)\n            sample_expected = sample_input[0].unsqueeze(0).to(device)\n            \n            \n            sample_outputs = model.generate(\n                input_ids=sample_input_ids,\n                do_sample=True,\n                top_k=50,\n                max_length=max_seq,\n                top_p=0.95,\n                num_return_sequences=1\n            )\n            \n            \n            gen_sample_input = tokenizer.decode(sample_input_ids[0], skip_special_tokens=False).replace(\"<|startoftext|>\", \"\\n\")\n            gen_sample_input = gen_sample_input.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").replace(\"<sep>\", \"\\t\")\n            \n            gen_expected_input = tokenizer.decode(sample_expected[0], skip_special_tokens=False).replace(\"<|startoftext|>\", \"\\n\")\n            gen_expected_input = gen_expected_input.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").replace(\"<sep>\", \"\\t\")\n            \n            for i, sample_output in enumerate(sample_outputs):\n                gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False).replace(\"<|startoftext|>\", \"\\n\")\n                gen_sample_output = gen_sample_output.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").replace(\"<sep>\", \"\\t\")\n\n                \n                print(f'Example output for:\\nreal:\\t{gen_expected_input}\\n\\noutput:\\t{gen_sample_output}')\n\n            model.train()\n\n            \n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss / len(train_dataloader)\n\n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print()\n    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n    print()\n\n    t0 = time.time()\n\n    model.eval()\n\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in tqdm(validation_dataloader, total=len(validation_dataloader), position=0):\n\n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n\n        with torch.no_grad():\n\n            outputs = model(b_input_ids, attention_mask=b_masks, labels=b_labels)\n\n            loss = outputs[0]\n\n        batch_loss = loss.item()\n        total_eval_loss += batch_loss\n\n    avg_val_loss = total_eval_loss / len(validation_dataloader)\n\n    validation_time = format_time(time.time() - t0)\n\n    print()\n    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')\n    print()\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(f'Total training took {format_time(time.time()-total_t0)}')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m-X-YWgdIgcT","outputId":"5f651302-9136-4b19-fcf5-4b80717dea09","execution":{"iopub.status.busy":"2024-01-17T19:02:38.657195Z","iopub.execute_input":"2024-01-17T19:02:38.657465Z","iopub.status.idle":"2024-01-17T19:55:54.908673Z","shell.execute_reply.started":"2024-01-17T19:02:38.657440Z","shell.execute_reply":"2024-01-17T19:55:54.907158Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"  0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Beginning epoch 1 of 5\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1000/2791 [03:37<06:35,  4.53it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 1000 of 2791. Loss:3.022099733352661. Time:0:03:38\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1001/2791 [03:38<11:39,  2.56it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tچو رومی پس اندر هم آواز شد\tچو گشتاسپ زان جایگه باز شد\nبر قیصر آمد سپه تاخته\tبه پیروزی و گردن افراخته\n\noutput:\tچو رومی پس اندر هم آواز شد\tچو گشتاسپ زان جایگه باز شد\nسکندر چو کشتی ز لشکر بگفت\tپراندیشه شد و درد و آرام و جفت\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2000/2791 [07:19<02:55,  4.50it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 2000 of 2791. Loss:2.7014827728271484. Time:0:07:20\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2001/2791 [07:20<04:53,  2.69it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tچو شاه جهان نامه هاشان بخواند\tز گفتارشان در شگفتی بماند\nبه نامه هر اندام را زو یکی\tصفت کرده بودند لیک اندکی\n\noutput:\tچو شاه جهان نامه هاشان بخواند\tز گفتارشان در شگفتی بماند\nازان پس همه مهتران و به داد\tنماندی بد او را که دارد نژاد\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2791/2791 [10:15<00:00,  4.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage Training Loss: 3.266349526469915. Epoch time: 0:10:16\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 311/311 [00:18<00:00, 16.43it/s]\n 20%|██        | 1/5 [10:34<42:18, 634.62s/it]","output_type":"stream"},{"name":"stdout","text":"\nValidation loss: 2.591286558813604. Validation Time: 0:00:19\n\nBeginning epoch 2 of 5\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1000/2791 [03:42<06:39,  4.48it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 1000 of 2791. Loss:2.5276567935943604. Time:0:03:43\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1001/2791 [03:43<10:55,  2.73it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tز دژ تا بر او دو فرسنگ بود\tدل مهتران زان سخن تنگ بود\nهمی هر کسی خواندند آفرین\tز دادار بر فر شاه زمین\n\noutput:\tز دژ تا بر او دو فرسنگ بود\tدل مهتران زان سخن تنگ بود\nهمی رفت پویان به نزدیک اوی\tچو تاریک شد جان تاریک اوی\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2000/2791 [07:25<02:55,  4.51it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 2000 of 2791. Loss:2.129385471343994. Time:0:07:26\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2001/2791 [07:26<04:56,  2.67it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tبه قیصر بگو گر نداری خرد\tز رای تو مغز تو کیفر برد\nاگر شیر جنگی بتازد بگور\tکنامش کند گور و هم آب شور\n\noutput:\tبه قیصر بگو گر نداری خرد\tز رای تو مغز تو کیفر برد\nکسی زین سخن گفت و او را بگوی\tچنین گفت کین رستم پوی پوی\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1000/2791 [03:42<06:35,  4.53it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 1000 of 2791. Loss:1.8285737037658691. Time:0:03:43\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1001/2791 [03:43<10:59,  2.71it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tپسر داشتی یک گرانمایه مرد\tجهاندیده و دیده هر گرم و سرد\nسواری جهاندیده نامش کهرم\tرسیده بسی بر سرش سرد و گرم\n\noutput:\tپسر داشتی یک گرانمایه مرد\tجهاندیده و دیده هر گرم و سرد\nچنین گفت رستم ز راه سپاه\tکه این رزمجویی نیاید به راه\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2000/2791 [07:25<02:54,  4.54it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 2000 of 2791. Loss:1.862695574760437. Time:0:07:26\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2001/2791 [07:26<04:52,  2.70it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tبفرمود تا زاد فرخ برفت\tبه نزدیک آن لشکر شاه تفت\nچنین بود پیغام نزد سپاه\tکه از پیش بودی مرا نیک خواه\n\noutput:\tبفرمود تا زاد فرخ برفت\tبه نزدیک آن لشکر شاه تفت\nبفرمود بهرام کو را سپرد\tسر از رزم کسری پر از آب کرد\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2791/2791 [10:21<00:00,  4.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage Training Loss: 1.8432859960998185. Epoch time: 0:10:22\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 311/311 [00:18<00:00, 16.45it/s]\n 60%|██████    | 3/5 [31:56<21:18, 639.33s/it]","output_type":"stream"},{"name":"stdout","text":"\nValidation loss: 1.8994825043478962. Validation Time: 0:00:19\n\nBeginning epoch 4 of 5\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1000/2791 [03:42<06:40,  4.47it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 1000 of 2791. Loss:1.4615832567214966. Time:0:03:42\n","output_type":"stream"},{"name":"stderr","text":" 36%|███▌      | 1001/2791 [03:42<10:57,  2.72it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tزده حربه ها را بن اندر زمین\tهمان نیز ژوپین و شمشیر کین\nبه خاشاک کرده سر چاه کور\tکه مردم ندیدی نه چشم ستور\n\noutput:\tزده حربه ها را بن اندر زمین\tهمان نیز ژوپین و شمشیر کین\nتو گر باهشی مشمر او را به دام\tوگر چند باشد ترا روی دام\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2000/2791 [07:24<02:55,  4.50it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 2000 of 2791. Loss:1.4794018268585205. Time:0:07:25\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2001/2791 [07:25<05:05,  2.59it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tفرستادگان سپهدار چین\tز پیش جهانجوی شاه زمین\nبرفتند هر دو شده خاکسار\tجهاندارشان رانده و کرده خوار\n\noutput:\tفرستادگان سپهدار چین\tز پیش جهانجوی شاه زمین\nازویست شادی امید شاهی\tبه هر کارداری و هر کارداری\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2791/2791 [10:20<00:00,  4.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage Training Loss: 1.4015129145137386. Epoch time: 0:10:21\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 311/311 [00:18<00:00, 16.44it/s]\n 80%|████████  | 4/5 [42:35<10:39, 639.41s/it]","output_type":"stream"},{"name":"stdout","text":"\nValidation loss: 1.64917712694579. Validation Time: 0:00:19\n\nBeginning epoch 5 of 5\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2000/2791 [07:25<02:56,  4.49it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\nBatch 2000 of 2791. Loss:1.1357437372207642. Time:0:07:25\n","output_type":"stream"},{"name":"stderr","text":" 72%|███████▏  | 2001/2791 [07:25<05:04,  2.60it/s]","output_type":"stream"},{"name":"stdout","text":"Example output for:\nreal:\tپدر چون ورا دید خیره بماند\tجهان آفرین را نهانی بخواند\nبدو گفت ای شسته مغز از خرد\tز پرگوهران این کی اندر خورد\n\noutput:\tپدر چون ورا دید خیره بماند\tجهان آفرین را نهانی بخواند\nگروی زره را گره تا گره\tبه گردنکشان و به گرز و گره\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2791/2791 [10:21<00:00,  4.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nAverage Training Loss: 1.0959248333283114. Epoch time: 0:10:22\n\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 311/311 [00:18<00:00, 16.44it/s]\n100%|██████████| 5/5 [53:16<00:00, 639.24s/it]","output_type":"stream"},{"name":"stdout","text":"\nValidation loss: 1.5425211325335733. Validation Time: 0:00:19\n\nTotal training took 0:53:16\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Some Examples:","metadata":{}},{"cell_type":"code","source":"model.eval()\n\nfor i in range(10):\n    sample_input = val_dataset[random.randint(0, len(val_dataset))]\n    sample_input_tokens = sample_input[0][:torch.where(sample_input[0] == 6)[0][0]+1]\n    sample_input_ids = sample_input_tokens.unsqueeze(0).to(device)# sample_input_ids.to(device)\n    sample_expected = sample_input[0].unsqueeze(0).to(device)\n\n    sample_outputs = model.generate(\n            input_ids=sample_input_ids,\n            do_sample=True,\n            top_k=50,\n            max_length=max_seq,\n            top_p=0.95,\n            num_return_sequences=1\n    )\n\n\n    gen_sample_input = tokenizer.decode(sample_input_ids[0], skip_special_tokens=False).replace(\"<|startoftext|>\", \"\\n\\t\")\n    gen_sample_input = gen_sample_input.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").replace(\"<sep>\", \"\\t\")\n\n    gen_expected_input = tokenizer.decode(sample_expected[0], skip_special_tokens=False).replace(\"<|startoftext|>\", \"\\n\\t\")\n    gen_expected_input = gen_expected_input.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").replace(\"<sep>\", \"\\t\")\n\n    for i, sample_output in enumerate(sample_outputs):\n        gen_sample_output = tokenizer.decode(sample_output, skip_special_tokens=False).replace(\"<|startoftext|>\", \"\\n\\t\")\n        gen_sample_output = gen_sample_output.replace(\"<s>\", \"\").replace(\"</s>\", \"\").replace(\"<pad>\", \"\").replace(\"<sep>\", \"\\t\")\n\n\n    print(f'real:\\t{gen_expected_input}\\n\\noutput:\\t{gen_sample_output}\\n\\n\\n\\n')\n","metadata":{"execution":{"iopub.status.busy":"2024-01-17T20:04:15.602361Z","iopub.execute_input":"2024-01-17T20:04:15.603264Z","iopub.status.idle":"2024-01-17T20:04:19.377941Z","shell.execute_reply.started":"2024-01-17T20:04:15.603227Z","shell.execute_reply":"2024-01-17T20:04:19.376812Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nThe attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tچو خورشید تابان ز بالا بگشت\tچه آن دژ نمود و چه آن پهن دشت\n\tبکشتند ازیشان فزون از شمار\tهمی دود از آتش برآمد چوقار\n\noutput:\tچو خورشید تابان ز بالا بگشت\tچه آن دژ نمود و چه آن پهن دشت\n\tبخورد و بینداخت شیر دلیر\tبپیش اندرون باره چون نره شیر\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tبه هشتم بیامد به دشت شکار\tخود و روزبه با سواری هزار\n\tهمه دشت یکسر پر از گور دید\tز قربان کمان کیان برکشید\n\noutput:\tبه هشتم بیامد به دشت شکار\tخود و روزبه با سواری هزار\n\tسپهدار ترکان بنه برنهاد\tیکی درع پرمایه بر سر نهاد\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tگر ایدونک دانی که من کردم این\tمرا خواند باید جهان آفرین\n\tز گوینده بپذیر به دین اوی\tبیاموز ازو راه و آیین اوی\n\noutput:\tگر ایدونک دانی که من کردم این\tمرا خواند باید جهان آفرین\n\tگر از نیکوی بایدم هردیم\tولیکن شنیدن مر او را همیم\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tپراگنده گشتند چون تیره شد\tسرمیگساران ز می خیره شد\n\tچو برزد سنان آفتاب بلند\tشب تیره گشت از درفشش نژند\n\noutput:\tپراگنده گشتند چون تیره شد\tسرمیگساران ز می خیره شد\n\tچنین گفت کامشب نباید غنود\tسپهبد همی نیزه باید بسود\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tز دریا به دریا سپه گسترید\tکه جایی کسی روی هامون ندید\n\tدو لشکر چو تنگ اندر آمد به گرد\tزمین شد سیاه و هوا لاژورد\n\noutput:\tز دریا به دریا سپه گسترید\tکه جایی کسی روی هامون ندید\n\tبدان کارزاری که یابند هیچ\tز هر سو بسی رنج کوتاه هیچ\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tگر از گفتهٔ خویش باز آید اوی\tبنزدیک ما رزم ساز آید اوی\n\tبفتراک بر بسته دارم کمند\tکجا ژنده پیل اندرآرم ببند\n\noutput:\tگر از گفتهٔ خویش باز آید اوی\tبنزدیک ما رزم ساز آید اوی\n\tبرآنم که او سر ز فرمان شاه\tبیاری بیاید بدین رزمگاه\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tکنون با تو ای پهلوان سپاه\tیکی دیگر افگند بازی براه\n\tجز از رنگ و چاره نداند همی\tز دانش سخن برفشاند همی\n\noutput:\tکنون با تو ای پهلوان سپاه\tیکی دیگر افگند بازی براه\n\tبه نزدیک او شد که کاووس شاه\tفزون کرد سوی سکندر نگاه\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tوگر من نیایم چو گودرز و گیو\tبخواهد ز تو کینهٔ پور نیو\n\tبرآمد برین کار یک روز و شب\tو زین گفته بر شاه نگشاد لب\n\noutput:\tوگر من نیایم چو گودرز و گیو\tبخواهد ز تو کینهٔ پور نیو\n\tمنوچهر بر میسره جای داشت\tز زابل به آمل همی جای داشت\n\n\n\n\n","output_type":"stream"},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"real:\tبیایند هر سه به نزدیک من\tشود روشن این شهر تاریک من\n\tشود شادمان دل به دیدارشان\tببینم روانهای بیدارشان\n\noutput:\tبیایند هر سه به نزدیک من\tشود روشن این شهر تاریک من\n\tسکندر بدو گفت کای نامجوی\tدو لشکر بروی اندر آریم روی\n\n\n\n\nreal:\tفرود آمد از باره گرگین چو گرد\tسر اندریمان ز تن دور کرد\n\tبفتراک بربست و خود برنشست\tنوند سوار نبرده بدست\n\noutput:\tفرود آمد از باره گرگین چو گرد\tسر اندریمان ز تن دور کرد\n\tبرفتند زان بوم تا مرز روم\tپراگنده گشتند زان مرز و بوم\n\n\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\noutput_dir = './content/'\nos.makedirs(output_dir, exist_ok=True)\n\ntorch.save(model, \"./content/GPT_Poet.pth\")\ntorch.save(tokenizer, \"./content/tokenizer.pth\")\ntorch.save(configuration, \"./content/config.pth\")\n\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\nconfiguration.save_pretrained(output_dir)\n","metadata":{"id":"b7s2FziqJn1x","execution":{"iopub.status.busy":"2024-01-17T20:14:35.648179Z","iopub.execute_input":"2024-01-17T20:14:35.648980Z","iopub.status.idle":"2024-01-17T20:14:37.798125Z","shell.execute_reply.started":"2024-01-17T20:14:35.648942Z","shell.execute_reply":"2024-01-17T20:14:37.797010Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"!zip -r content.zip content","metadata":{"execution":{"iopub.status.busy":"2024-01-17T20:15:51.261987Z","iopub.execute_input":"2024-01-17T20:15:51.262565Z","iopub.status.idle":"2024-01-17T20:16:44.779224Z","shell.execute_reply.started":"2024-01-17T20:15:51.262511Z","shell.execute_reply":"2024-01-17T20:16:44.777959Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: content/ (stored 0%)\n  adding: content/config.json (deflated 51%)\n  adding: content/vocab.json (deflated 73%)\n  adding: content/tokenizer.json (deflated 79%)\n  adding: content/special_tokens_map.json (deflated 79%)\n  adding: content/tokenizer.pth (deflated 83%)\n  adding: content/GPT_Poet.pth (deflated 10%)\n  adding: content/merges.txt (deflated 74%)\n  adding: content/added_tokens.json (stored 0%)\n  adding: content/tokenizer_config.json (deflated 85%)\n  adding: content/gpt2-fa-poetry/ (stored 0%)\n  adding: content/gpt2-fa-poetry/tokenizer.json (deflated 79%)\n  adding: content/model.safetensors (deflated 7%)\n  adding: content/config.pth (deflated 47%)\n  adding: content/generation_config.json (deflated 24%)\n","output_type":"stream"}]}]}